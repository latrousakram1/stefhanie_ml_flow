# ğŸ“Š PrÃ©vision de la Facture Ã‰lectrique RÃ©sidentielle (QuÃ©bec)

## ğŸ§  Description du projet

Ce projet vise Ã  prÃ©dire le montant mensuel de la facture dâ€™Ã©lectricitÃ© rÃ©sidentielle Ã  partir de donnÃ©es historiques de consommation par rÃ©gion.

Le pipeline couvre lâ€™ensemble du cycle machine learning :

- Chargement et nettoyage des donnÃ©es  
- Feature engineering temporel  
- Split temporel (train/test)  
- Pipelines de prÃ©traitement  
- EntraÃ®nement de plusieurs modÃ¨les  
- Optimisation dâ€™hyperparamÃ¨tres  
- Suivi des expÃ©riences  
- Comparaison des performances  
- Sauvegarde du meilleur modÃ¨le  

---

## ğŸš€ Technologies utilisÃ©es

- Python 3.12  
- NumPy  
- Pandas  
- scikit-learn  
- MLflow  
- Optuna  
- Matplotlib  
- Seaborn  
- Joblib  

---

## ğŸ“ Structure du projet

```text
.
â”œâ”€â”€ historique-consommation-secteur-activite-ra-mois.csv
â”œâ”€â”€ modele_final_gradientboosting.pkl
â”œâ”€â”€ mae_comparaison_base.png
â”œâ”€â”€ comparaison_MAE.png
â”œâ”€â”€ comparaison_RMSE.png
â”œâ”€â”€ comparaison_RÂ².png
â”œâ”€â”€ notebook.ipynb
â””â”€â”€ README.md
âš™ï¸ Installation
pip install -U scikit-learn numpy scipy pandas mlflow optuna matplotlib seaborn joblib
ğŸ“¥ DonnÃ©es
Le fichier CSV contient :

RÃ©gion administrative

Date (annÃ©e/mois)

Secteur dâ€™activitÃ©

Consommation totale (kWh)

Seul le secteur rÃ©sidentiel est conservÃ©.

ğŸ›  Feature Engineering
Variables temporelles
mois

annÃ©e

saison_froide

Historique consommation
lag1 (M-1)

lag3 (M-3)

roll3 (moyenne mobile 3 mois)

Variables synthÃ©tiques
TempÃ©rature moyenne simulÃ©e

Tarif saisonnier

Interaction consommation Ã— tempÃ©rature

Variable cible
Montant_facture = Consommation_kWh Ã— Tarif
ğŸ”„ Split temporel
Train : avant janvier 2023

Test : janvier 2023 et aprÃ¨s

Cela simule un scÃ©nario rÃ©el de prÃ©diction future.

ğŸ§© PrÃ©processing
Pipeline :

StandardScaler â†’ variables numÃ©riques

OneHotEncoder â†’ rÃ©gions

Le tout encapsulÃ© dans un ColumnTransformer.

ğŸ¤– ModÃ¨les Ã©valuÃ©s
RÃ©gression linÃ©aire

Ridge Regression

Random Forest Regressor

Histogram Gradient Boosting Regressor

Chaque modÃ¨le est intÃ©grÃ© dans un Pipeline complet :

PrÃ©processing â†’ ModÃ¨le
ğŸ“ˆ MÃ©triques
MAE

RMSE

RÂ²

ğŸ“Š RÃ©sultats initiaux
ModÃ¨le	MAE
Linear	~1.56M
Ridge	~1.63M
RandomForest	~735k
GradientBoosting	~936k
RandomForest et GradientBoosting sont retenus pour optimisation.

ğŸ” Optimisation
RandomForest (GridSearchCV)
n_estimators

max_depth

min_samples_split

GradientBoosting (Optuna)
max_iter

learning_rate

max_depth

ğŸ† Performances finales
ModÃ¨le	Version	MAE
RandomForest	Base	735k
RandomForest	OptimisÃ©	735k
GradientBoosting	Base	936k
GradientBoosting	OptimisÃ©	918k
Le GradientBoosting optimisÃ© est sÃ©lectionnÃ© comme modÃ¨le final.

ğŸ’¾ Sauvegarde du modÃ¨le
Le modÃ¨le final est sauvegardÃ© :

modele_final_gradientboosting.pkl
Chargement :

import joblib
model = joblib.load("modele_final_gradientboosting.pkl")
ğŸ“‰ Visualisations
Comparaison MAE / RMSE / RÂ²

PrÃ©dictions vs RÃ©alitÃ©

RÃ©sidus

Ces graphiques permettent lâ€™analyse des erreurs et la validation visuelle.

ğŸ§ª Tracking MLflow
Chaque run enregistre :

hyperparamÃ¨tres

mÃ©triques

graphiques

modÃ¨le

Objectifs :

reproductibilitÃ©

audit ML

comparaison dâ€™expÃ©riences

ğŸ“Œ Conclusion
Pipeline ML complet orientÃ© sÃ©ries temporelles avec :

Feature engineering avancÃ©

Pipelines sklearn propres

Optimisation automatique

Tracking MLflow

Le modÃ¨le final atteint un RÂ² â‰ˆ 0.9986.

ğŸ‘¤ CrÃ©ateur
Latrous Akram
ğŸ“§ latrousakram@gmail.com
